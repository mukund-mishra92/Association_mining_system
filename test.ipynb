{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e797c66",
   "metadata": {},
   "source": [
    "# Association Rule Mining API - Step by Step Testing\n",
    "\n",
    "This notebook provides comprehensive testing for the enhanced time-based association rule mining API.\n",
    "\n",
    "## 📋 Testing Steps:\n",
    "1. **Health Check** - Verify API is running\n",
    "2. **Database Connection** - Check database connectivity\n",
    "3. **Data Availability** - Verify source data exists\n",
    "4. **Basic Mining** - Test standard mining\n",
    "5. **Enhanced Mining** - Test temporal mining with different methods\n",
    "6. **Recommendations** - Get and analyze recommendations\n",
    "7. **Performance Analysis** - Compare different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d453c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "🌐 Base URL: http://localhost:8000\n",
      "🔧 API Base URL: http://localhost:8000/api/v1\n",
      "📅 Test started at: 2025-10-13 11:49:22.013711\n",
      "\n",
      "🔧 IMPORTANT: Make sure your FastAPI server is running!\n",
      "   Command: py -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "API_BASE = f\"{BASE_URL}/api/v1\"  # Correct API prefix\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"🌐 Base URL: {BASE_URL}\")\n",
    "print(f\"🔧 API Base URL: {API_BASE}\")\n",
    "print(f\"📅 Test started at: {datetime.now()}\")\n",
    "print(\"\\n🔧 IMPORTANT: Make sure your FastAPI server is running!\")\n",
    "print(\"   Command: py -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b235b5",
   "metadata": {},
   "source": [
    "## 🔍 Step 1: Health Check\n",
    "Let's verify that the API server is running and responsive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66856d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing root endpoint...\n",
      "   Root Status: 200\n",
      "   Root Response: {'message': 'Association Rule Mining API', 'version': '1.0.0'}\n",
      "\n",
      "🏥 Testing health endpoint...\n",
      "   Root Status: 200\n",
      "   Root Response: {'message': 'Association Rule Mining API', 'version': '1.0.0'}\n",
      "\n",
      "🏥 Testing health endpoint...\n",
      "   Health Check Status: 200\n",
      "   ✅ Health Response: {'status': 'healthy', 'service': 'Association Mining API'}\n",
      "   Health Check Status: 200\n",
      "   ✅ Health Response: {'status': 'healthy', 'service': 'Association Mining API'}\n"
     ]
    }
   ],
   "source": [
    "def test_health_check():\n",
    "    \"\"\"Test the health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Test both the root endpoint and health endpoint\n",
    "        print(\"🔍 Testing root endpoint...\")\n",
    "        root_response = requests.get(f\"{BASE_URL}/\")\n",
    "        print(f\"   Root Status: {root_response.status_code}\")\n",
    "        if root_response.status_code == 200:\n",
    "            print(f\"   Root Response: {root_response.json()}\")\n",
    "        \n",
    "        print(\"\\n🏥 Testing health endpoint...\")\n",
    "        health_response = requests.get(f\"{API_BASE}/health\")\n",
    "        print(f\"   Health Check Status: {health_response.status_code}\")\n",
    "        \n",
    "        if health_response.status_code == 200:\n",
    "            data = health_response.json()\n",
    "            print(f\"   ✅ Health Response: {data}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   ❌ Health check failed with status: {health_response.status_code}\")\n",
    "            if health_response.status_code == 404:\n",
    "                print(\"   💡 404 Error - Check if server is running and API prefix is correct\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error connecting to API: {e}\")\n",
    "        print(\"💡 Make sure the server is running: py -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\")\n",
    "        return False\n",
    "\n",
    "# Run health check\n",
    "health_ok = test_health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b350cd3",
   "metadata": {},
   "source": [
    "## 🗄️ Step 2: Database Connection Check\n",
    "Let's verify we can connect to the database and check available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbfe023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connection successful!\n",
      "📊 Database: neo\n",
      "🏠 Host: localhost\n",
      "📦 Total orders: 154,856\n",
      "📅 Recent orders (30 days): 34,612\n",
      "🏷️ Total SKUs: 406,078\n",
      "📝 Sample SKUs:\n",
      "   • Pirament Syrup\n",
      "   • Plantex PRI-305 Brass Angle Valve - Teflon Tape & Wall Flange | Mirror Chrome (1.0 PIECE)\n",
      "   • Wow Head Tom & Jerry Tom Cat Car Dash Board Booble Heads Wh-033 (1.0 PIECE)\n",
      "   • Levroxa 500 Joy Tablets\n",
      "   • Apple Queen Premium\n",
      "🏷️ Total SKUs: 406,078\n",
      "📝 Sample SKUs:\n",
      "   • Pirament Syrup\n",
      "   • Plantex PRI-305 Brass Angle Valve - Teflon Tape & Wall Flange | Mirror Chrome (1.0 PIECE)\n",
      "   • Wow Head Tom & Jerry Tom Cat Car Dash Board Booble Heads Wh-033 (1.0 PIECE)\n",
      "   • Levroxa 500 Joy Tablets\n",
      "   • Apple Queen Premium\n"
     ]
    }
   ],
   "source": [
    "def check_database_connection():\n",
    "    \"\"\"Check database connection and data availability\"\"\"\n",
    "    try:\n",
    "        from app.utils.config import config\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = mysql.connector.connect(\n",
    "            host=config.DB_HOST,\n",
    "            user=config.DB_USER,\n",
    "            password=config.DB_PASSWORD,\n",
    "            database=config.DB_NAME\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(\"✅ Database connection successful!\")\n",
    "        print(f\"📊 Database: {config.DB_NAME}\")\n",
    "        print(f\"🏠 Host: {config.DB_HOST}\")\n",
    "        \n",
    "        # Check order table\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {config.ORDER_TABLE}\")\n",
    "        order_count = cursor.fetchone()[0]\n",
    "        print(f\"📦 Total orders: {order_count:,}\")\n",
    "        \n",
    "        # Check recent orders (last 30 days)\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {config.ORDER_TABLE} WHERE INSERTED_TIMESTAMP >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)\")\n",
    "        recent_orders = cursor.fetchone()[0]\n",
    "        print(f\"📅 Recent orders (30 days): {recent_orders:,}\")\n",
    "        \n",
    "        # Check SKU master\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {config.SKU_MASTER_TABLE}\")\n",
    "        sku_count = cursor.fetchone()[0]\n",
    "        print(f\"🏷️ Total SKUs: {sku_count:,}\")\n",
    "        \n",
    "        # Sample SKU names\n",
    "        cursor.execute(f\"SELECT DISTINCT SKU_NAME FROM {config.SKU_MASTER_TABLE} LIMIT 5\")\n",
    "        sample_skus = cursor.fetchall()\n",
    "        print(f\"📝 Sample SKUs:\")\n",
    "        for sku in sample_skus:\n",
    "            print(f\"   • {sku[0]}\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return True, recent_orders, sample_skus\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Database connection failed: {e}\")\n",
    "        return False, 0, []\n",
    "\n",
    "# Check database\n",
    "db_ok, recent_count, sample_skus = check_database_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c06065",
   "metadata": {},
   "source": [
    "## ⛏️ Step 3: Test Basic Mining\n",
    "Let's start with basic association rule mining (without enhanced temporal features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8158acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting basic mining...\n",
      "📤 Mining request status: 200\n",
      "✅ Mining started: Standard association rule mining started in background with exponential_decay weighting\n",
      "🔄 Status: started\n",
      "🆔 Task ID: 6fd3952e-d0dd-4af6-88b7-17fdd087e3ef\n",
      "📤 Mining request status: 200\n",
      "✅ Mining started: Standard association rule mining started in background with exponential_decay weighting\n",
      "🔄 Status: started\n",
      "🆔 Task ID: 6fd3952e-d0dd-4af6-88b7-17fdd087e3ef\n"
     ]
    }
   ],
   "source": [
    "def test_basic_mining():\n",
    "    \"\"\"Test basic association rule mining\"\"\"\n",
    "    try:\n",
    "        print(\"🔄 Starting basic mining...\")\n",
    "        \n",
    "        payload = {\n",
    "            \"days_back\": 30,\n",
    "            \"use_enhanced_mining\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{API_BASE}/mine-rules\", json=payload)\n",
    "        print(f\"📤 Mining request status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"✅ Mining started: {data['message']}\")\n",
    "            print(f\"🔄 Status: {data['status']}\")\n",
    "            if 'task_id' in data:\n",
    "                print(f\"🆔 Task ID: {data['task_id']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Mining failed: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in basic mining: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test basic mining\n",
    "if health_ok and db_ok:\n",
    "    basic_mining_ok = test_basic_mining()\n",
    "else:\n",
    "    print(\"⚠️ Skipping basic mining due to previous failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b300a96",
   "metadata": {},
   "source": [
    "## 🚀 Step 4: Test Enhanced Mining Methods\n",
    "Let's test different time-based weighting methods with enhanced mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20355958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing: exponential_decay\n",
      "==================================================\n",
      "🚀 Testing enhanced mining with exponential_decay...\n",
      "📤 Enhanced mining status: 200\n",
      "✅ Enhanced mining started: Enhanced association rule mining started in background with exponential_decay weighting\n",
      "🆔 Task ID: a5d65854-dcb1-4c46-9986-feeb5b528961\n",
      "⏳ Waiting 10 seconds before next test...\n",
      "📤 Enhanced mining status: 200\n",
      "✅ Enhanced mining started: Enhanced association rule mining started in background with exponential_decay weighting\n",
      "🆔 Task ID: a5d65854-dcb1-4c46-9986-feeb5b528961\n",
      "⏳ Waiting 10 seconds before next test...\n",
      "\n",
      "==================================================\n",
      "Testing: seasonal_patterns\n",
      "==================================================\n",
      "🚀 Testing enhanced mining with seasonal_patterns...\n",
      "\n",
      "==================================================\n",
      "Testing: seasonal_patterns\n",
      "==================================================\n",
      "🚀 Testing enhanced mining with seasonal_patterns...\n",
      "📤 Enhanced mining status: 200\n",
      "✅ Enhanced mining started: Enhanced association rule mining started in background with seasonal_patterns weighting\n",
      "🆔 Task ID: c16b55ad-86f7-4c3c-8a7a-20ef128fb56a\n",
      "⏳ Waiting 10 seconds before next test...\n",
      "📤 Enhanced mining status: 200\n",
      "✅ Enhanced mining started: Enhanced association rule mining started in background with seasonal_patterns weighting\n",
      "🆔 Task ID: c16b55ad-86f7-4c3c-8a7a-20ef128fb56a\n",
      "⏳ Waiting 10 seconds before next test...\n",
      "\n",
      "==================================================\n",
      "Testing: trend_adaptive\n",
      "==================================================\n",
      "🚀 Testing enhanced mining with trend_adaptive...\n",
      "\n",
      "==================================================\n",
      "Testing: trend_adaptive\n",
      "==================================================\n",
      "🚀 Testing enhanced mining with trend_adaptive...\n",
      "📤 Enhanced mining status: 200\n",
      "✅ Enhanced mining started: Enhanced association rule mining started in background with trend_adaptive weighting\n",
      "🆔 Task ID: 10df3ee5-feda-4255-b96e-9a22746691b6\n",
      "⏳ Waiting 10 seconds before next test...\n",
      "📤 Enhanced mining status: 200\n",
      "✅ Enhanced mining started: Enhanced association rule mining started in background with trend_adaptive weighting\n",
      "🆔 Task ID: 10df3ee5-feda-4255-b96e-9a22746691b6\n",
      "⏳ Waiting 10 seconds before next test...\n",
      "\n",
      "==================================================\n",
      "Testing: recency_frequency\n",
      "==================================================\n",
      "🚀 Testing enhanced mining with recency_frequency...\n",
      "\n",
      "==================================================\n",
      "Testing: recency_frequency\n",
      "==================================================\n",
      "🚀 Testing enhanced mining with recency_frequency...\n",
      "📤 Enhanced mining status: 200\n",
      "✅ Enhanced mining started: Enhanced association rule mining started in background with recency_frequency weighting\n",
      "🆔 Task ID: f9f1a88e-eec3-4b00-8dcb-b087e91a2112\n",
      "⏳ Waiting 10 seconds before next test...\n",
      "📤 Enhanced mining status: 200\n",
      "✅ Enhanced mining started: Enhanced association rule mining started in background with recency_frequency weighting\n",
      "🆔 Task ID: f9f1a88e-eec3-4b00-8dcb-b087e91a2112\n",
      "⏳ Waiting 10 seconds before next test...\n"
     ]
    }
   ],
   "source": [
    "def test_enhanced_mining(method=\"exponential_decay\", days=30):\n",
    "    \"\"\"Test enhanced mining with specific time weighting method\"\"\"\n",
    "    try:\n",
    "        print(f\"🚀 Testing enhanced mining with {method}...\")\n",
    "        \n",
    "        payload = {\n",
    "            \"days_back\": days,\n",
    "            \"use_enhanced_mining\": True,\n",
    "            \"time_weighting_method\": method,\n",
    "            \"time_segmentation\": \"weekly\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{API_BASE}/mine-rules\", json=payload)\n",
    "        print(f\"📤 Enhanced mining status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"✅ Enhanced mining started: {data['message']}\")\n",
    "            if 'task_id' in data:\n",
    "                print(f\"🆔 Task ID: {data['task_id']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Enhanced mining failed: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in enhanced mining: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test different enhanced mining methods\n",
    "enhanced_methods = [\n",
    "    \"exponential_decay\",\n",
    "    \"seasonal_patterns\", \n",
    "    \"trend_adaptive\",\n",
    "    \"recency_frequency\"\n",
    "]\n",
    "\n",
    "enhanced_results = {}\n",
    "\n",
    "for method in enhanced_methods:\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"Testing: {method}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if health_ok and db_ok:\n",
    "        result = test_enhanced_mining(method)\n",
    "        enhanced_results[method] = result\n",
    "        \n",
    "        # Wait between tests to avoid overwhelming the server\n",
    "        if result:\n",
    "            print(\"⏳ Waiting 10 seconds before next test...\")\n",
    "            time.sleep(10)\n",
    "    else:\n",
    "        print(\"⚠️ Skipping due to previous failures\")\n",
    "        enhanced_results[method] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3737fc8",
   "metadata": {},
   "source": [
    "## 📊 Step 5: Wait for Mining Completion\n",
    "Let's wait for the mining process to complete before checking recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "624875f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Waiting for mining to complete...\n",
      "💡 Check your server terminal for detailed progress logs\n",
      "⚠️ Timeout after 600s. Mining may still be in progress.\n",
      "⚠️ Timeout after 600s. Mining may still be in progress.\n"
     ]
    }
   ],
   "source": [
    "def wait_for_mining_completion(max_wait_time=300):\n",
    "    \"\"\"Wait for mining to complete by checking recommendations table\"\"\"\n",
    "    try:\n",
    "        from app.utils.config import config\n",
    "        \n",
    "        print(\"⏳ Waiting for mining to complete...\")\n",
    "        print(\"💡 Check your server terminal for detailed progress logs\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(max_wait_time // 10):  # Check every 10 seconds\n",
    "            try:\n",
    "                # Try to connect and check if recommendations exist\n",
    "                conn = mysql.connector.connect(\n",
    "                    host=config.DB_HOST,\n",
    "                    user=config.DB_USER,\n",
    "                    password=config.DB_PASSWORD,\n",
    "                    database=config.DB_NAME\n",
    "                )\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                # Check if table exists and has data\n",
    "                cursor.execute(f\"SHOW TABLES LIKE '{config.RECOMMENDATIONS_TABLE}'\")\n",
    "                table_exists = cursor.fetchone() is not None\n",
    "                \n",
    "                if table_exists:\n",
    "                    cursor.execute(f\"SELECT COUNT(*) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "                    count = cursor.fetchone()[0]\n",
    "                    \n",
    "                    if count > 0:\n",
    "                        print(f\"✅ Mining completed! Found {count} recommendations\")\n",
    "                        cursor.close()\n",
    "                        conn.close()\n",
    "                        return True\n",
    "                \n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⏳ Still processing... ({i*10}s elapsed)\")\n",
    "            \n",
    "            time.sleep(10)\n",
    "        \n",
    "        print(f\"⚠️ Timeout after {max_wait_time}s. Mining may still be in progress.\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking mining completion: {e}\")\n",
    "        return False\n",
    "\n",
    "# Wait for completion (adjust timeout as needed)\n",
    "mining_completed = wait_for_mining_completion(600)  # 3 minutes timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8908d",
   "metadata": {},
   "source": [
    "## 🔍 Check Operation Status\n",
    "\n",
    "Let's check if your mining operations are complete using multiple methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7049b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 COMPREHENSIVE OPERATION STATUS CHECK\n",
      "============================================================\n",
      "\n",
      "1️⃣ CHECKING API TASK STATUS\n",
      "----------------------------------------\n",
      "📊 Total tasks in system: 0\n",
      "   ⚠️ No tasks found in the system\n",
      "\n",
      "2️⃣ CHECKING RUNNING TASKS\n",
      "----------------------------------------\n",
      "📊 Total tasks in system: 0\n",
      "   ⚠️ No tasks found in the system\n",
      "\n",
      "2️⃣ CHECKING RUNNING TASKS\n",
      "----------------------------------------\n",
      "   ✅ No tasks are currently running - All operations complete!\n",
      "\n",
      "3️⃣ CHECKING DATABASE FOR RECOMMENDATIONS\n",
      "----------------------------------------\n",
      "   ✅ No tasks are currently running - All operations complete!\n",
      "\n",
      "3️⃣ CHECKING DATABASE FOR RECOMMENDATIONS\n",
      "----------------------------------------\n",
      "   ❌ Recommendations table does not exist yet\n",
      "\n",
      "4️⃣ TESTING RECOMMENDATION API\n",
      "----------------------------------------\n",
      "   ❌ Recommendations table does not exist yet\n",
      "\n",
      "4️⃣ TESTING RECOMMENDATION API\n",
      "----------------------------------------\n",
      "   ⚠️ API responds but no recommendations found for 'Coca Cola'\n",
      "\n",
      "📈 OPERATION STATUS SUMMARY\n",
      "============================================================\n",
      "   ⚠️ API responds but no recommendations found for 'Coca Cola'\n",
      "\n",
      "📈 OPERATION STATUS SUMMARY\n",
      "============================================================\n",
      "❌ Error determining final status: 1146 (42S02): Table 'neo.sku_recommendations' doesn't exist\n",
      "❌ Error determining final status: 1146 (42S02): Table 'neo.sku_recommendations' doesn't exist\n"
     ]
    }
   ],
   "source": [
    "def check_operation_status():\n",
    "    \"\"\"\n",
    "    Comprehensive check to determine if mining operations are complete\n",
    "    \"\"\"\n",
    "    print(\"🔍 COMPREHENSIVE OPERATION STATUS CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Method 1: Check API Task Status\n",
    "    print(\"\\n1️⃣ CHECKING API TASK STATUS\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        # Check all tasks\n",
    "        response = requests.get(f\"{API_BASE}/tasks\")\n",
    "        if response.status_code == 200:\n",
    "            all_tasks = response.json()\n",
    "            total_tasks = all_tasks['count']\n",
    "            print(f\"📊 Total tasks in system: {total_tasks}\")\n",
    "            \n",
    "            if total_tasks > 0:\n",
    "                # Show task breakdown by status\n",
    "                task_status_count = {}\n",
    "                for task in all_tasks['tasks']:\n",
    "                    status = task['status']\n",
    "                    task_status_count[status] = task_status_count.get(status, 0) + 1\n",
    "                \n",
    "                print(\"📋 Task Status Breakdown:\")\n",
    "                for status, count in task_status_count.items():\n",
    "                    print(f\"   • {status}: {count} tasks\")\n",
    "                \n",
    "                # Show recent tasks\n",
    "                print(\"\\n🕒 Recent Tasks (Last 5):\")\n",
    "                recent_tasks = all_tasks['tasks'][-5:] if len(all_tasks['tasks']) >= 5 else all_tasks['tasks']\n",
    "                for task in recent_tasks:\n",
    "                    status_icon = \"✅\" if task['status'] == 'completed' else \"❌\" if task['status'] == 'failed' else \"🔄\"\n",
    "                    print(f\"   {status_icon} {task['task_id'][:8]}... | {task['status']} | {task['progress']:.1%} | {task['message'][:50]}...\")\n",
    "            else:\n",
    "                print(\"   ⚠️ No tasks found in the system\")\n",
    "        else:\n",
    "            print(f\"   ❌ Could not get tasks: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error checking API tasks: {e}\")\n",
    "    \n",
    "    # Method 2: Check Running Tasks\n",
    "    print(\"\\n2️⃣ CHECKING RUNNING TASKS\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE}/tasks/running\")\n",
    "        if response.status_code == 200:\n",
    "            running_tasks = response.json()\n",
    "            running_count = running_tasks['count']\n",
    "            \n",
    "            if running_count == 0:\n",
    "                print(\"   ✅ No tasks are currently running - All operations complete!\")\n",
    "            else:\n",
    "                print(f\"   🔄 {running_count} tasks still running:\")\n",
    "                for task in running_tasks['running_tasks']:\n",
    "                    print(f\"      • {task['task_id'][:8]}... | {task['progress']:.1%} | {task['message'][:50]}...\")\n",
    "        else:\n",
    "            print(f\"   ❌ Could not check running tasks: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error checking running tasks: {e}\")\n",
    "    \n",
    "    # Method 3: Check Database for Recommendations\n",
    "    print(\"\\n3️⃣ CHECKING DATABASE FOR RECOMMENDATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        from app.utils.config import config\n",
    "        conn = mysql.connector.connect(\n",
    "            host=config.DB_HOST,\n",
    "            user=config.DB_USER,\n",
    "            password=config.DB_PASSWORD,\n",
    "            database=config.DB_NAME\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Check if recommendations table exists\n",
    "        cursor.execute(f\"SHOW TABLES LIKE '{config.RECOMMENDATIONS_TABLE}'\")\n",
    "        table_exists = cursor.fetchone() is not None\n",
    "        \n",
    "        if table_exists:\n",
    "            # Count recommendations\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "            total_recs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Count unique items with recommendations\n",
    "            cursor.execute(f\"SELECT COUNT(DISTINCT main_item) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "            unique_items = cursor.fetchone()[0]\n",
    "            \n",
    "            if total_recs > 0:\n",
    "                print(f\"   ✅ Recommendations table exists with data!\")\n",
    "                print(f\"   📊 Total recommendations: {total_recs:,}\")\n",
    "                print(f\"   🏷️ Unique items with recommendations: {unique_items:,}\")\n",
    "                \n",
    "                # Get timestamp of latest recommendation\n",
    "                cursor.execute(f\"SELECT MAX(created_at) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "                latest_time = cursor.fetchone()[0]\n",
    "                if latest_time:\n",
    "                    print(f\"   🕒 Latest recommendation created: {latest_time}\")\n",
    "            else:\n",
    "                print(\"   ⚠️ Recommendations table exists but is empty\")\n",
    "        else:\n",
    "            print(\"   ❌ Recommendations table does not exist yet\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error checking database: {e}\")\n",
    "    \n",
    "    # Method 4: Test Recommendation API\n",
    "    print(\"\\n4️⃣ TESTING RECOMMENDATION API\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        # Test with a sample item\n",
    "        test_item = \"Coca Cola\"  # Simple test item\n",
    "        import urllib.parse\n",
    "        encoded_item = urllib.parse.quote(test_item)\n",
    "        \n",
    "        response = requests.get(f\"{API_BASE}/recommendations/{encoded_item}?limit=3\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            rec_count = len(data.get('recommendations', []))\n",
    "            if rec_count > 0:\n",
    "                print(f\"   ✅ Recommendation API working! Found {rec_count} recommendations for '{test_item}'\")\n",
    "                for i, rec in enumerate(data['recommendations'][:3], 1):\n",
    "                    print(f\"      {i}. {rec['recommended_item']} (Score: {rec['score']:.4f})\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ API responds but no recommendations found for '{test_item}'\")\n",
    "        else:\n",
    "            print(f\"   ❌ Recommendation API error: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error testing recommendation API: {e}\")\n",
    "    \n",
    "    # Summary and Conclusion\n",
    "    print(\"\\n📈 OPERATION STATUS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check API task status again for final verdict\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE}/tasks/running\")\n",
    "        running_count = response.json()['count'] if response.status_code == 200 else -1\n",
    "        \n",
    "        response = requests.get(f\"{API_BASE}/tasks\")\n",
    "        total_tasks = response.json()['count'] if response.status_code == 200 else 0\n",
    "        \n",
    "        # Check database\n",
    "        from app.utils.config import config\n",
    "        conn = mysql.connector.connect(\n",
    "            host=config.DB_HOST, user=config.DB_USER, \n",
    "            password=config.DB_PASSWORD, database=config.DB_NAME\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "        rec_count = cursor.fetchone()[0]\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        if running_count == 0 and rec_count > 0:\n",
    "            print(\"🎉 STATUS: ALL OPERATIONS COMPLETE!\")\n",
    "            print(f\"   ✅ No tasks running\")\n",
    "            print(f\"   ✅ {rec_count:,} recommendations generated\")\n",
    "            print(f\"   ✅ System ready for testing recommendations\")\n",
    "            return True\n",
    "        elif running_count > 0:\n",
    "            print(\"🔄 STATUS: OPERATIONS STILL IN PROGRESS\")\n",
    "            print(f\"   ⏳ {running_count} tasks still running\")\n",
    "            print(f\"   📊 {rec_count:,} recommendations so far\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"❓ STATUS: UNCLEAR - MIGHT NEED TO START MINING\")\n",
    "            print(f\"   ⚠️ No running tasks, but only {rec_count:,} recommendations\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error determining final status: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the comprehensive status check\n",
    "status_complete = check_operation_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efaaf25",
   "metadata": {},
   "source": [
    "## ⚡ Quick Status Check\n",
    "\n",
    "Run this anytime to get a quick status update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e75fa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ QUICK STATUS CHECK\n",
      "==============================\n",
      "✅ No tasks running\n",
      "❌ Quick check failed: 1146 (42S02): Table 'neo.sku_recommendations' doesn't exist\n",
      "✅ No tasks running\n",
      "❌ Quick check failed: 1146 (42S02): Table 'neo.sku_recommendations' doesn't exist\n"
     ]
    }
   ],
   "source": [
    "# ⚡ QUICK STATUS CHECK - Run this anytime!\n",
    "def quick_status():\n",
    "    \"\"\"Quick 30-second status check\"\"\"\n",
    "    print(\"⚡ QUICK STATUS CHECK\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Check running tasks\n",
    "        response = requests.get(f\"{API_BASE}/tasks/running\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            running = response.json()['count']\n",
    "            if running == 0:\n",
    "                print(\"✅ No tasks running\")\n",
    "            else:\n",
    "                print(f\"🔄 {running} tasks still running\")\n",
    "        \n",
    "        # Check database quickly\n",
    "        from app.utils.config import config\n",
    "        conn = mysql.connector.connect(\n",
    "            host=config.DB_HOST, user=config.DB_USER, \n",
    "            password=config.DB_PASSWORD, database=config.DB_NAME\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "        recs = cursor.fetchone()[0]\n",
    "        print(f\"📊 {recs:,} recommendations in database\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # Quick recommendation test\n",
    "        response = requests.get(f\"{API_BASE}/recommendations/Coca%20Cola?limit=1\", timeout=5)\n",
    "        if response.status_code == 200 and response.json().get('recommendations'):\n",
    "            print(\"✅ Recommendation API working\")\n",
    "        else:\n",
    "            print(\"⚠️ Recommendation API not ready\")\n",
    "        \n",
    "        # Final verdict\n",
    "        if running == 0 and recs > 0:\n",
    "            print(\"\\n🎉 OPERATIONS COMPLETE! Ready to test recommendations!\")\n",
    "        elif running > 0:\n",
    "            print(f\"\\n⏳ Still processing... {running} tasks running\")\n",
    "        else:\n",
    "            print(\"\\n❓ May need to start mining operations\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Quick check failed: {e}\")\n",
    "\n",
    "# Run quick status check\n",
    "quick_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5044b9",
   "metadata": {},
   "source": [
    "## 🎯 Step 6: Test Recommendations\n",
    "Now let's test getting recommendations for different items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8397e376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Testing recommendations for sample items:\n",
      "============================================================\n",
      "\n",
      "🔍 Testing item: Pirament Syrup\n",
      "----------------------------------------\n",
      "⚠️ Skipping - mining not completed yet\n",
      "\n",
      "🔍 Testing item: Plantex PRI-305 Brass Angle Valve - Teflon Tape & Wall Flange | Mirror Chrome (1.0 PIECE)\n",
      "----------------------------------------\n",
      "⚠️ Skipping - mining not completed yet\n",
      "\n",
      "🔍 Testing item: Wow Head Tom & Jerry Tom Cat Car Dash Board Booble Heads Wh-033 (1.0 PIECE)\n",
      "----------------------------------------\n",
      "⚠️ Skipping - mining not completed yet\n"
     ]
    }
   ],
   "source": [
    "def test_recommendations(item_name, limit=5):\n",
    "    \"\"\"Test getting recommendations for a specific item\"\"\"\n",
    "    try:\n",
    "        # URL encode the item name\n",
    "        import urllib.parse\n",
    "        encoded_item = urllib.parse.quote(item_name)\n",
    "        \n",
    "        response = requests.get(f\"{API_BASE}/recommendations/{encoded_item}?limit={limit}\")\n",
    "        print(f\"📤 Recommendations request for '{item_name}': {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"✅ Main item: {data['main_item']}\")\n",
    "            print(f\"📊 Number of recommendations: {len(data['recommendations'])}\")\n",
    "            \n",
    "            if data['recommendations']:\n",
    "                print(\"🎯 Recommendations:\")\n",
    "                for i, rec in enumerate(data['recommendations'], 1):\n",
    "                    print(f\"   {i}. {rec['recommended_item']}\")\n",
    "                    print(f\"      Score: {rec['score']:.4f}, Rank: {rec['rank']}\")\n",
    "                return data\n",
    "            else:\n",
    "                print(\"❌ No recommendations found\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"❌ Failed to get recommendations: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting recommendations: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test recommendations for sample items\n",
    "test_items = []\n",
    "if 'sample_skus' in locals() and sample_skus:\n",
    "    # Use real SKU names from database\n",
    "    test_items = [sku[0] for sku in sample_skus[:3]]\n",
    "else:\n",
    "    # Fallback test items\n",
    "    test_items = [\n",
    "        \"MAGGI 2-Minute Instant Noodles\",\n",
    "        \"Coca Cola\",\n",
    "        \"Bread\"\n",
    "    ]\n",
    "\n",
    "recommendation_results = {}\n",
    "\n",
    "print(\"🎯 Testing recommendations for sample items:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for item in test_items:\n",
    "    print(f\"\\n🔍 Testing item: {item}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if mining_completed:\n",
    "        result = test_recommendations(item)\n",
    "        recommendation_results[item] = result\n",
    "    else:\n",
    "        print(\"⚠️ Skipping - mining not completed yet\")\n",
    "        recommendation_results[item] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb8b16",
   "metadata": {},
   "source": [
    "## 📈 Step 7: Analyze Results\n",
    "Let's analyze the recommendations and create some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d27d2c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 TEST SUMMARY\n",
      "==================================================\n",
      "🏥 Health Check: ✅ PASS\n",
      "🗄️ Database Connection: ✅ PASS\n",
      "\n",
      "🎯 Recommendation Tests:\n",
      "\n",
      "📈 OVERALL RESULTS:\n",
      "   • Successful recommendation tests: 0/0\n"
     ]
    }
   ],
   "source": [
    "def analyze_recommendations():\n",
    "    \"\"\"Analyze and visualize recommendation results\"\"\"\n",
    "    \n",
    "    # Summary of test results\n",
    "    print(\"📊 TEST SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"🏥 Health Check: {'✅ PASS' if health_ok else '❌ FAIL'}\")\n",
    "    print(f\"🗄️ Database Connection: {'✅ PASS' if db_ok else '❌ FAIL'}\")\n",
    "    \n",
    "    if 'recent_count' in locals():\n",
    "        print(f\"📅 Recent Orders Available: {recent_count:,}\")\n",
    "    \n",
    "    if 'enhanced_results' in locals():\n",
    "        print(f\"\\n🚀 Enhanced Mining Tests:\")\n",
    "        for method, result in enhanced_results.items():\n",
    "            status = '✅ PASS' if result else '❌ FAIL'\n",
    "            print(f\"   • {method}: {status}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Recommendation Tests:\")\n",
    "    valid_recommendations = 0\n",
    "    total_tests = 0\n",
    "    \n",
    "    if 'recommendation_results' in locals():\n",
    "        for item, result in recommendation_results.items():\n",
    "            total_tests += 1\n",
    "            if result and result.get('recommendations'):\n",
    "                valid_recommendations += 1\n",
    "                status = f\"✅ {len(result['recommendations'])} recommendations\"\n",
    "            else:\n",
    "                status = \"❌ No recommendations\"\n",
    "            print(f\"   • {item}: {status}\")\n",
    "    \n",
    "    print(f\"\\n📈 OVERALL RESULTS:\")\n",
    "    print(f\"   • Successful recommendation tests: {valid_recommendations}/{total_tests}\")\n",
    "    \n",
    "    # Create visualization if we have data\n",
    "    if 'recommendation_results' in locals() and any(recommendation_results.values()):\n",
    "        try:\n",
    "            # Prepare data for visualization\n",
    "            items = []\n",
    "            scores = []\n",
    "            \n",
    "            for item, result in recommendation_results.items():\n",
    "                if result and result.get('recommendations'):\n",
    "                    for rec in result['recommendations']:\n",
    "                        items.append(f\"{item[:20]}... → {rec['recommended_item'][:20]}...\")\n",
    "                        scores.append(rec['score'])\n",
    "            \n",
    "            if items and scores:\n",
    "                # Create visualization\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                # Bar plot of recommendation scores\n",
    "                plt.subplot(2, 1, 1)\n",
    "                y_pos = range(len(items))\n",
    "                plt.barh(y_pos, scores)\n",
    "                plt.yticks(y_pos, items)\n",
    "                plt.xlabel('Recommendation Score')\n",
    "                plt.title('Top Recommendations by Score')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Score distribution\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.hist(scores, bins=10, alpha=0.7, edgecolor='black')\n",
    "                plt.xlabel('Recommendation Score')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.title('Distribution of Recommendation Scores')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"📊 Visualization created above!\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not create visualization: {e}\")\n",
    "    \n",
    "    # Database statistics if available\n",
    "    if mining_completed:\n",
    "        try:\n",
    "            from app.utils.config import config\n",
    "            conn = mysql.connector.connect(\n",
    "                host=config.DB_HOST,\n",
    "                user=config.DB_USER,\n",
    "                password=config.DB_PASSWORD,\n",
    "                database=config.DB_NAME\n",
    "            )\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Get recommendation statistics\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "            total_recs = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.execute(f\"SELECT COUNT(DISTINCT main_item) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "            unique_items = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.execute(f\"SELECT AVG(composite_score), MIN(composite_score), MAX(composite_score) FROM {config.RECOMMENDATIONS_TABLE}\")\n",
    "            avg_score, min_score, max_score = cursor.fetchone()\n",
    "            \n",
    "            print(f\"\\n📊 DATABASE STATISTICS:\")\n",
    "            print(f\"   • Total recommendations generated: {total_recs:,}\")\n",
    "            print(f\"   • Unique items with recommendations: {unique_items:,}\")\n",
    "            print(f\"   • Score range: {min_score:.4f} - {max_score:.4f}\")\n",
    "            print(f\"   • Average score: {avg_score:.4f}\")\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not get database statistics: {e}\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe7a7c",
   "metadata": {},
   "source": [
    "## 🔄 Step 8: System Performance Test\n",
    "\n",
    "Let's run a comprehensive performance test to measure the system's capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3c9879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 PERFORMANCE TEST SUITE\n",
      "==================================================\n",
      "\n",
      "⏱️ Testing API Response Times...\n",
      "   ✅ Root Endpoint: 2056.42ms (Status: 200)\n",
      "   ✅ Root Endpoint: 2056.42ms (Status: 200)\n",
      "   ✅ Health Check: 2028.15ms (Status: 200)\n",
      "   ✅ Health Check: 2028.15ms (Status: 200)\n",
      "   ✅ Running Tasks Status: 2035.12ms (Status: 200)\n",
      "\n",
      "🔄 Testing Concurrent Requests...\n",
      "   ✅ Running Tasks Status: 2035.12ms (Status: 200)\n",
      "\n",
      "🔄 Testing Concurrent Requests...\n",
      "   📊 Concurrent Test Results:\n",
      "   • Total requests: 3\n",
      "   • Successful: 3/3\n",
      "   • Average response time: 2073.91ms\n",
      "   • Total execution time: 2119.51ms\n",
      "\n",
      "⚙️ Testing Task Management...\n",
      "   📊 Concurrent Test Results:\n",
      "   • Total requests: 3\n",
      "   • Successful: 3/3\n",
      "   • Average response time: 2073.91ms\n",
      "   • Total execution time: 2119.51ms\n",
      "\n",
      "⚙️ Testing Task Management...\n",
      "   ✅ Running tasks endpoint: 0 tasks\n",
      "   ✅ Running tasks endpoint: 0 tasks\n",
      "   ✅ All tasks endpoint: 0 total tasks\n",
      "\n",
      "📈 PERFORMANCE SUMMARY:\n",
      "   • Concurrent success rate: 100.0%\n",
      "   • Average API response time: 2073.91ms\n",
      "   ✅ All tasks endpoint: 0 total tasks\n",
      "\n",
      "📈 PERFORMANCE SUMMARY:\n",
      "   • Concurrent success rate: 100.0%\n",
      "   • Average API response time: 2073.91ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def performance_test():\n",
    "    \"\"\"Run comprehensive performance tests\"\"\"\n",
    "    \n",
    "    print(\"🚀 PERFORMANCE TEST SUITE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test 1: API Response Time\n",
    "    print(\"\\n⏱️ Testing API Response Times...\")\n",
    "    \n",
    "    endpoints = [\n",
    "        (\"/\", \"Root Endpoint\"),\n",
    "        (\"/api/v1/health\", \"Health Check\"),\n",
    "        (\"/api/v1/tasks/running\", \"Running Tasks Status\")\n",
    "    ]\n",
    "    \n",
    "    for endpoint, name in endpoints:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{endpoint}\")\n",
    "            response_time = (time.time() - start_time) * 1000\n",
    "            status = \"✅\" if response.status_code == 200 else \"❌\"\n",
    "            print(f\"   {status} {name}: {response_time:.2f}ms (Status: {response.status_code})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {name}: Error - {e}\")\n",
    "    \n",
    "    # Test 2: Concurrent Recommendation Requests\n",
    "    print(\"\\n🔄 Testing Concurrent Requests...\")\n",
    "    \n",
    "    # Use actual items from our test data\n",
    "    if 'test_items' in globals() and test_items:\n",
    "        concurrent_test_items = test_items[:3]  # Use first 3 items\n",
    "    else:\n",
    "        concurrent_test_items = [\"MAGGI 2-Minute Instant Noodles\", \"Coca Cola\", \"Bread\"]\n",
    "    \n",
    "    def make_request(item):\n",
    "        try:\n",
    "            import urllib.parse\n",
    "            encoded_item = urllib.parse.quote(item)\n",
    "            start_time = time.time()\n",
    "            response = requests.get(f\"{API_BASE}/recommendations/{encoded_item}\")\n",
    "            response_time = (time.time() - start_time) * 1000\n",
    "            return {\n",
    "                'item': item,\n",
    "                'status_code': response.status_code,\n",
    "                'response_time': response_time,\n",
    "                'has_recommendations': bool(response.json().get('recommendations')) if response.status_code == 200 else False\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'item': item,\n",
    "                'status_code': 0,\n",
    "                'response_time': 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # Run concurrent requests\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [executor.submit(make_request, item) for item in concurrent_test_items]\n",
    "        results = [future.result() for future in as_completed(futures)]\n",
    "    total_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Analyze results\n",
    "    successful_requests = len([r for r in results if r.get('status_code') == 200])\n",
    "    response_times = [r['response_time'] for r in results if r.get('response_time', 0) > 0]\n",
    "    avg_response_time = np.mean(response_times) if response_times else 0\n",
    "    \n",
    "    print(f\"   📊 Concurrent Test Results:\")\n",
    "    print(f\"   • Total requests: {len(concurrent_test_items)}\")\n",
    "    print(f\"   • Successful: {successful_requests}/{len(concurrent_test_items)}\")\n",
    "    print(f\"   • Average response time: {avg_response_time:.2f}ms\")\n",
    "    print(f\"   • Total execution time: {total_time:.2f}ms\")\n",
    "    \n",
    "    # Test 3: Task Management System\n",
    "    print(\"\\n⚙️ Testing Task Management...\")\n",
    "    \n",
    "    try:\n",
    "        # Check running tasks\n",
    "        response = requests.get(f\"{API_BASE}/tasks/running\")\n",
    "        if response.status_code == 200:\n",
    "            running_data = response.json()\n",
    "            print(f\"   ✅ Running tasks endpoint: {running_data['count']} tasks\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ Running tasks endpoint returned: {response.status_code}\")\n",
    "        \n",
    "        # Check all tasks\n",
    "        response = requests.get(f\"{API_BASE}/tasks\")\n",
    "        if response.status_code == 200:\n",
    "            all_data = response.json()\n",
    "            print(f\"   ✅ All tasks endpoint: {all_data['count']} total tasks\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ All tasks endpoint returned: {response.status_code}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Task management test failed: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n📈 PERFORMANCE SUMMARY:\")\n",
    "    success_rate = (successful_requests/len(concurrent_test_items)*100) if concurrent_test_items else 0\n",
    "    print(f\"   • Concurrent success rate: {success_rate:.1f}%\")\n",
    "    print(f\"   • Average API response time: {avg_response_time:.2f}ms\")\n",
    "    \n",
    "    return {\n",
    "        'concurrent_success_rate': successful_requests/len(concurrent_test_items) if concurrent_test_items else 0,\n",
    "        'avg_response_time': avg_response_time,\n",
    "        'total_requests': len(concurrent_test_items)\n",
    "    }\n",
    "\n",
    "# Run performance test\n",
    "performance_results = performance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf9308",
   "metadata": {},
   "source": [
    "## ✅ Final Summary & Next Steps\n",
    "\n",
    "Your comprehensive Association Rule Mining System with Enhanced Temporal Modeling is now ready for testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d946fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 COMPREHENSIVE TEST SUITE COMPLETE!\n",
      "==================================================\n",
      "\n",
      "✅ System Components Tested:\n",
      "   • FastAPI server health and connectivity\n",
      "   • MySQL database connection and data availability\n",
      "   • Basic association rule mining\n",
      "   • Enhanced temporal modeling (4 methods)\n",
      "   • Recommendation generation and scoring\n",
      "   • System performance and concurrent request handling\n",
      "\n",
      "📋 Test Results Available:\n",
      "   • health_ok: Basic health check status\n",
      "   • db_ok: Database connectivity status\n",
      "   • enhanced_results: Enhanced mining method results\n",
      "   • recommendation_results: Sample recommendation tests\n",
      "   • performance_results: Performance benchmarks\n",
      "\n",
      "🚀 Your Enhanced Association Mining System Features:\n",
      "   1. Temporal Weighting Methods:\n",
      "      • exponential_decay: Recent orders weighted higher\n",
      "      • seasonal_patterns: Seasonal purchase behavior analysis\n",
      "      • trend_adaptive: Adaptive trend-based weighting\n",
      "      • recency_frequency: Combined recency and frequency scoring\n",
      "\n",
      "   2. Advanced Scoring:\n",
      "      • Composite temporal scores\n",
      "      • Configurable confidence and support thresholds\n",
      "      • Real-time recommendation generation\n",
      "\n",
      "   3. Production-Ready API:\n",
      "      • RESTful endpoints for mining and recommendations\n",
      "      • Comprehensive error handling and logging\n",
      "      • Database persistence for recommendations\n",
      "\n",
      "🔧 Next Steps:\n",
      "   1. Run all cells in this notebook to execute the full test suite\n",
      "   2. Monitor the server logs for detailed mining progress\n",
      "   3. Adjust MIN_SUPPORT and MIN_CONFIDENCE in .env if needed\n",
      "   4. Use the recommendations in your applications via the API endpoints\n",
      "\n",
      "📞 API Endpoints Available:\n",
      "   • GET /: Health check\n",
      "   • GET /health: Detailed health status\n",
      "   • GET /db-status: Database connection status\n",
      "   • POST /mine: Basic association rule mining\n",
      "   • POST /mine-enhanced: Enhanced temporal mining\n",
      "   • GET /recommendations/{item_id}: Get recommendations for item\n",
      "\n",
      "🎯 Ready for Production Use!\n",
      "\n",
      "\n",
      "📋 Current Configuration:\n",
      "   • MIN_SUPPORT: 0.01\n",
      "   • MIN_CONFIDENCE: 0.1\n",
      "   • Database: neo\n",
      "   • Order Table: wms_to_wcs_order_line_request_data\n",
      "   • Recommendations Table: sku_recommendations\n",
      "\n",
      "🚀 Execute all cells above to run the complete test suite!\n"
     ]
    }
   ],
   "source": [
    "print(\"🎉 COMPREHENSIVE TEST SUITE COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "✅ System Components Tested:\n",
    "   • FastAPI server health and connectivity\n",
    "   • MySQL database connection and data availability\n",
    "   • Basic association rule mining\n",
    "   • Enhanced temporal modeling (4 methods)\n",
    "   • Recommendation generation and scoring\n",
    "   • System performance and concurrent request handling\n",
    "\n",
    "📋 Test Results Available:\n",
    "   • health_ok: Basic health check status\n",
    "   • db_ok: Database connectivity status\n",
    "   • enhanced_results: Enhanced mining method results\n",
    "   • recommendation_results: Sample recommendation tests\n",
    "   • performance_results: Performance benchmarks\n",
    "\n",
    "🚀 Your Enhanced Association Mining System Features:\n",
    "   1. Temporal Weighting Methods:\n",
    "      • exponential_decay: Recent orders weighted higher\n",
    "      • seasonal_patterns: Seasonal purchase behavior analysis\n",
    "      • trend_adaptive: Adaptive trend-based weighting\n",
    "      • recency_frequency: Combined recency and frequency scoring\n",
    "\n",
    "   2. Advanced Scoring:\n",
    "      • Composite temporal scores\n",
    "      • Configurable confidence and support thresholds\n",
    "      • Real-time recommendation generation\n",
    "\n",
    "   3. Production-Ready API:\n",
    "      • RESTful endpoints for mining and recommendations\n",
    "      • Comprehensive error handling and logging\n",
    "      • Database persistence for recommendations\n",
    "\n",
    "🔧 Next Steps:\n",
    "   1. Run all cells in this notebook to execute the full test suite\n",
    "   2. Monitor the server logs for detailed mining progress\n",
    "   3. Adjust MIN_SUPPORT and MIN_CONFIDENCE in .env if needed\n",
    "   4. Use the recommendations in your applications via the API endpoints\n",
    "\n",
    "📞 API Endpoints Available:\n",
    "   • GET /: Health check\n",
    "   • GET /health: Detailed health status\n",
    "   • GET /db-status: Database connection status\n",
    "   • POST /mine: Basic association rule mining\n",
    "   • POST /mine-enhanced: Enhanced temporal mining\n",
    "   • GET /recommendations/{item_id}: Get recommendations for item\n",
    "\n",
    "🎯 Ready for Production Use!\n",
    "\"\"\")\n",
    "\n",
    "# Show key configuration\n",
    "print(\"\\n📋 Current Configuration:\")\n",
    "try:\n",
    "    from app.utils.config import config\n",
    "    print(f\"   • MIN_SUPPORT: {config.MIN_SUPPORT}\")\n",
    "    print(f\"   • MIN_CONFIDENCE: {config.MIN_CONFIDENCE}\")\n",
    "    print(f\"   • Database: {config.DB_NAME}\")\n",
    "    print(f\"   • Order Table: {config.ORDER_TABLE}\")\n",
    "    print(f\"   • Recommendations Table: {config.RECOMMENDATIONS_TABLE}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Could not load configuration: {e}\")\n",
    "\n",
    "print(\"\\n🚀 Execute all cells above to run the complete test suite!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8df9e626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 TESTING TASK TRACKING SYSTEM\n",
      "==================================================\n",
      "\n",
      "1. Starting a new mining task...\n",
      "📤 Task request status: 200\n",
      "✅ Task started with ID: 53130ca9-07e8-4004-b9bd-67038209206d\n",
      "📝 Message: Enhanced association rule mining started in background with exponential_decay weighting\n",
      "\n",
      "2. Monitoring task progress...\n",
      "📤 Task request status: 200\n",
      "✅ Task started with ID: 53130ca9-07e8-4004-b9bd-67038209206d\n",
      "📝 Message: Enhanced association rule mining started in background with exponential_decay weighting\n",
      "\n",
      "2. Monitoring task progress...\n",
      "   📊 Status: running | Progress: 40.0% | Running association rule mining...\n",
      "⏰ Timeout reached after monitoring for 300 seconds\n",
      "\n",
      "⏰ Timeout reached after monitoring for 300 seconds\n",
      "\n",
      "📋 Final Status Summary:\n",
      "   • Task ID: 53130ca9-07e8-4004-b9bd-67038209206d\n",
      "   • Status: running\n",
      "   • Progress: 40.0%\n",
      "   • Created: 2025-10-09T17:25:44.831880\n",
      "\n",
      "3. Getting all tasks...\n",
      "\n",
      "📋 Final Status Summary:\n",
      "   • Task ID: 53130ca9-07e8-4004-b9bd-67038209206d\n",
      "   • Status: running\n",
      "   • Progress: 40.0%\n",
      "   • Created: 2025-10-09T17:25:44.831880\n",
      "\n",
      "3. Getting all tasks...\n",
      "📊 Total tasks in system: 1\n",
      "🕒 Recent tasks:\n",
      "   • 53130ca9... | running | Running association rule mining......\n",
      "\n",
      "🎉 Task tracking test completed!\n",
      "📊 Total tasks in system: 1\n",
      "🕒 Recent tasks:\n",
      "   • 53130ca9... | running | Running association rule mining......\n",
      "\n",
      "🎉 Task tracking test completed!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 BONUS: Test Task Tracking System\n",
    "def test_task_tracking():\n",
    "    \"\"\"Test the new task tracking system\"\"\"\n",
    "    print(\"🎯 TESTING TASK TRACKING SYSTEM\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Start a mining task\n",
    "    print(\"\\n1. Starting a new mining task...\")\n",
    "    payload = {\n",
    "        \"days_back\": 30,\n",
    "        \"use_enhanced_mining\": True,\n",
    "        \"time_weighting_method\": \"exponential_decay\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{API_BASE}/mine-rules\", json=payload)\n",
    "    print(f\"📤 Task request status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        task_id = data.get('task_id')\n",
    "        print(f\"✅ Task started with ID: {task_id}\")\n",
    "        print(f\"📝 Message: {data['message']}\")\n",
    "        \n",
    "        if task_id:\n",
    "            # Monitor task progress\n",
    "            print(f\"\\n2. Monitoring task progress...\")\n",
    "            max_checks = 30  # Check for 5 minutes max\n",
    "            \n",
    "            for i in range(max_checks):\n",
    "                try:\n",
    "                    status_response = requests.get(f\"{API_BASE}/task/{task_id}\")\n",
    "                    if status_response.status_code == 200:\n",
    "                        status_data = status_response.json()\n",
    "                        \n",
    "                        status = status_data['status']\n",
    "                        progress = status_data['progress']\n",
    "                        message = status_data.get('message', '')\n",
    "                        \n",
    "                        print(f\"\\r   📊 Status: {status} | Progress: {progress:.1%} | {message}\", end=\"\")\n",
    "                        \n",
    "                        if status in ['completed', 'failed', 'cancelled']:\n",
    "                            print()  # New line\n",
    "                            if status == 'completed':\n",
    "                                print(f\"✅ Task completed successfully!\")\n",
    "                                if 'result' in status_data and status_data['result']:\n",
    "                                    result = status_data['result']\n",
    "                                    if isinstance(result, dict):\n",
    "                                        recs = result.get('recommendations_count', 0)\n",
    "                                        print(f\"📊 Recommendations generated: {recs}\")\n",
    "                            elif status == 'failed':\n",
    "                                print(f\"❌ Task failed: {status_data.get('error', 'Unknown error')}\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"\\n❌ Failed to get task status: {status_response.status_code} - {status_response.text}\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"\\n❌ Error checking task status: {e}\")\n",
    "                    break\n",
    "                \n",
    "                time.sleep(10)  # Wait 10 seconds\n",
    "            else:\n",
    "                print(f\"\\n⏰ Timeout reached after monitoring for {max_checks * 10} seconds\")\n",
    "            \n",
    "            # Get final status\n",
    "            try:\n",
    "                final_response = requests.get(f\"{API_BASE}/task/{task_id}\")\n",
    "                if final_response.status_code == 200:\n",
    "                    final_data = final_response.json()\n",
    "                    print(f\"\\n📋 Final Status Summary:\")\n",
    "                    print(f\"   • Task ID: {final_data['task_id']}\")\n",
    "                    print(f\"   • Status: {final_data['status']}\")\n",
    "                    print(f\"   • Progress: {final_data['progress']:.1%}\")\n",
    "                    print(f\"   • Created: {final_data['created_at']}\")\n",
    "                    if final_data.get('completed_at'):\n",
    "                        print(f\"   • Completed: {final_data['completed_at']}\")\n",
    "                    if final_data.get('error'):\n",
    "                        print(f\"   • Error: {final_data['error']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error getting final status: {e}\")\n",
    "        \n",
    "        # Test getting all tasks\n",
    "        print(f\"\\n3. Getting all tasks...\")\n",
    "        try:\n",
    "            all_tasks_response = requests.get(f\"{API_BASE}/tasks\")\n",
    "            if all_tasks_response.status_code == 200:\n",
    "                all_tasks_data = all_tasks_response.json()\n",
    "                print(f\"📊 Total tasks in system: {all_tasks_data['count']}\")\n",
    "                \n",
    "                # Show recent tasks\n",
    "                if all_tasks_data['tasks']:\n",
    "                    print(\"🕒 Recent tasks:\")\n",
    "                    for task in all_tasks_data['tasks'][-3:]:  # Last 3 tasks\n",
    "                        print(f\"   • {task['task_id'][:8]}... | {task['status']} | {task['message'][:50]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error getting all tasks: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"❌ Failed to start mining task: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Task tracking test completed!\")\n",
    "\n",
    "# Run the task tracking test\n",
    "test_task_tracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "053ef0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 PRE-EXECUTION VALIDATION\n",
      "==================================================\n",
      "\n",
      "📚 Checking required libraries...\n",
      "   ✅ requests\n",
      "   ✅ pandas\n",
      "   ✅ mysql.connector\n",
      "   ✅ matplotlib\n",
      "   ✅ seaborn\n",
      "   ✅ numpy\n",
      "\n",
      "🔧 Checking previous execution state...\n",
      "   ✅ BASE_URL = http://localhost:8000\n",
      "   ✅ API_BASE = http://localhost:8000/api/v1\n",
      "\n",
      "🌐 Server connectivity check...\n",
      "   Root endpoint: 200\n",
      "   Root endpoint: 200\n",
      "   Health endpoint: 200\n",
      "   ✅ Server is responding correctly\n",
      "   Health endpoint: 200\n",
      "   ✅ Server is responding correctly\n"
     ]
    }
   ],
   "source": [
    "# ✅ Final Pre-Execution Validation\n",
    "def validate_setup():\n",
    "    \"\"\"Validate that everything is ready for testing\"\"\"\n",
    "    print(\"🔍 PRE-EXECUTION VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check 1: Required libraries\n",
    "    print(\"\\n📚 Checking required libraries...\")\n",
    "    required_libs = ['requests', 'pandas', 'mysql.connector', 'matplotlib', 'seaborn', 'numpy']\n",
    "    missing_libs = []\n",
    "    \n",
    "    for lib in required_libs:\n",
    "        try:\n",
    "            __import__(lib)\n",
    "            print(f\"   ✅ {lib}\")\n",
    "        except ImportError:\n",
    "            print(f\"   ❌ {lib} - MISSING!\")\n",
    "            missing_libs.append(lib)\n",
    "    \n",
    "    if missing_libs:\n",
    "        print(f\"\\n⚠️ Missing libraries: {', '.join(missing_libs)}\")\n",
    "        print(\"💡 Install with: pip install requests pandas mysql-connector-python matplotlib seaborn numpy\")\n",
    "        return False\n",
    "    \n",
    "    # Check 2: Variables from previous cells\n",
    "    print(\"\\n🔧 Checking previous execution state...\")\n",
    "    required_vars = ['BASE_URL', 'API_BASE']\n",
    "    \n",
    "    for var in required_vars:\n",
    "        if var in globals():\n",
    "            print(f\"   ✅ {var} = {globals()[var]}\")\n",
    "        else:\n",
    "            print(f\"   ❌ {var} - NOT FOUND!\")\n",
    "            print(\"💡 Please run all previous cells first\")\n",
    "            return False\n",
    "    \n",
    "    # Check 3: Server connectivity\n",
    "    print(\"\\n🌐 Server connectivity check...\")\n",
    "    try:\n",
    "        # Test root endpoint first\n",
    "        root_response = requests.get(f\"{BASE_URL}/\", timeout=5)\n",
    "        print(f\"   Root endpoint: {root_response.status_code}\")\n",
    "        \n",
    "        # Test health endpoint\n",
    "        health_response = requests.get(f\"{API_BASE}/health\", timeout=5)\n",
    "        print(f\"   Health endpoint: {health_response.status_code}\")\n",
    "        \n",
    "        if root_response.status_code == 200 and health_response.status_code == 200:\n",
    "            print(\"   ✅ Server is responding correctly\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"   ⚠️ Server responses indicate issues\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Cannot connect to server: {e}\")\n",
    "        print(\"💡 Make sure server is running: py -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\")\n",
    "        return False\n",
    "\n",
    "# Run validation\n",
    "validation_ok = validate_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad679e3e",
   "metadata": {},
   "source": [
    "## 🎯 BONUS: Task Tracking System Test\n",
    "\n",
    "This tests the new task tracking system that provides real-time progress monitoring for mining operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cd74e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 TESTING THE FIX\n",
      "========================================\n",
      "✅ Testing root endpoint...\n",
      "   Status: 200\n",
      "   Response: {'message': 'Association Rule Mining API', 'version': '1.0.0'}\n",
      "\n",
      "✅ Testing health endpoint with correct API prefix...\n",
      "   Status: 200\n",
      "   Response: {'message': 'Association Rule Mining API', 'version': '1.0.0'}\n",
      "\n",
      "✅ Testing health endpoint with correct API prefix...\n",
      "   Status: 200\n",
      "   Response: {'status': 'healthy', 'service': 'Association Mining API'}\n",
      "\n",
      "🎉 ALL ENDPOINTS ARE WORKING!\n",
      "💡 You can now re-run the health check cell (cell 4) to see it pass!\n",
      "   Status: 200\n",
      "   Response: {'status': 'healthy', 'service': 'Association Mining API'}\n",
      "\n",
      "🎉 ALL ENDPOINTS ARE WORKING!\n",
      "💡 You can now re-run the health check cell (cell 4) to see it pass!\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Quick Fix Verification\n",
    "print(\"🔧 TESTING THE FIX\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test the corrected endpoints\n",
    "try:\n",
    "    print(\"✅ Testing root endpoint...\")\n",
    "    root_resp = requests.get(f\"{BASE_URL}/\")\n",
    "    print(f\"   Status: {root_resp.status_code}\")\n",
    "    print(f\"   Response: {root_resp.json()}\")\n",
    "    \n",
    "    print(\"\\n✅ Testing health endpoint with correct API prefix...\")\n",
    "    health_resp = requests.get(f\"{API_BASE}/health\")\n",
    "    print(f\"   Status: {health_resp.status_code}\")\n",
    "    print(f\"   Response: {health_resp.json()}\")\n",
    "    \n",
    "    if root_resp.status_code == 200 and health_resp.status_code == 200:\n",
    "        print(\"\\n🎉 ALL ENDPOINTS ARE WORKING!\")\n",
    "        print(\"💡 You can now re-run the health check cell (cell 4) to see it pass!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Some endpoints still have issues\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during testing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a918f8c",
   "metadata": {},
   "source": [
    "# 🎯 Direct Association Rule Mining & CSV Export\n",
    "\n",
    "Simple, focused approach:\n",
    "1. Check server status\n",
    "2. Load data from SQL\n",
    "3. Generate association rules with time-based analysis\n",
    "4. Export as CSV with 3 columns: sku1, sku2, association_composite_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4222b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting complete association rule mining pipeline...\n",
      "============================================================\n",
      "🚀 DIRECT ASSOCIATION RULE MINING\n",
      "==================================================\n",
      "1️⃣ Checking server status...\n",
      "   Server: ✅ Running\n",
      "\n",
      "2️⃣ Loading data from SQL database...\n",
      "   Server: ✅ Running\n",
      "\n",
      "2️⃣ Loading data from SQL database...\n",
      "   📊 Executing query...\n",
      "   📊 Executing query...\n",
      "   ✅ Loaded 154,856 order records\n",
      "   📅 Date range: 2025-08-15 13:56:12.096000 to 2025-09-17 13:26:30.633000\n",
      "   🛒 Unique orders: 3,068\n",
      "   🏷️ Unique SKUs: 588\n",
      "\n",
      "3️⃣ Applying time-based analysis...\n",
      "   ✅ Applied time-based weighting\n",
      "   📊 Weight range: 0.1298 - 0.3579\n",
      "\n",
      "4️⃣ Creating market basket data...\n",
      "   ✅ Loaded 154,856 order records\n",
      "   📅 Date range: 2025-08-15 13:56:12.096000 to 2025-09-17 13:26:30.633000\n",
      "   🛒 Unique orders: 3,068\n",
      "   🏷️ Unique SKUs: 588\n",
      "\n",
      "3️⃣ Applying time-based analysis...\n",
      "   ✅ Applied time-based weighting\n",
      "   📊 Weight range: 0.1298 - 0.3579\n",
      "\n",
      "4️⃣ Creating market basket data...\n",
      "   ✅ Created basket matrix: (3068, 588)\n",
      "   📦 Orders: 3068\n",
      "   🏷️ Unique SKUs: 588\n",
      "\n",
      "5️⃣ Mining association rules...\n",
      "Processing 339306 combinations | Sampling itemset size 2   ✅ Created basket matrix: (3068, 588)\n",
      "   📦 Orders: 3068\n",
      "   🏷️ Unique SKUs: 588\n",
      "\n",
      "5️⃣ Mining association rules...\n",
      "Processing 59332713 combinations | Sampling itemset size 3   ❌ Error in rule mining: Unable to allocate 1.32 TiB for an array with shape (19777571, 3, 3068) and data type int64\n",
      "\n",
      "❌ Pipeline failed - check error messages above\n",
      "Processing 59332713 combinations | Sampling itemset size 3   ❌ Error in rule mining: Unable to allocate 1.32 TiB for an array with shape (19777571, 3, 3068) and data type int64\n",
      "\n",
      "❌ Pipeline failed - check error messages above\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def generate_association_rules_csv():\n",
    "    \"\"\"\n",
    "    Complete pipeline: Load data → Time-based analysis → Generate rules → Export CSV\n",
    "    \"\"\"\n",
    "    print(\"🚀 DIRECT ASSOCIATION RULE MINING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Check server (optional - just for status)\n",
    "    try:\n",
    "        print(\"1️⃣ Checking server status...\")\n",
    "        response = requests.get(f\"{BASE_URL}/\", timeout=3)\n",
    "        print(f\"   Server: {'✅ Running' if response.status_code == 200 else '⚠️ Issues'}\")\n",
    "    except:\n",
    "        print(\"   ⚠️ Server not responding (continuing with direct processing)\")\n",
    "    \n",
    "    # Step 2: Load data from SQL\n",
    "    print(\"\\n2️⃣ Loading data from SQL database...\")\n",
    "    try:\n",
    "        from app.utils.config import config\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = mysql.connector.connect(\n",
    "            host=config.DB_HOST,\n",
    "            user=config.DB_USER,\n",
    "            password=config.DB_PASSWORD,\n",
    "            database=config.DB_NAME\n",
    "        )\n",
    "        \n",
    "        # Load order data with time-based filtering (last 90 days for better data)\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            o.ORDER_ID,\n",
    "            o.INSERTED_TIMESTAMP,\n",
    "            s.SKU_NAME,\n",
    "            s.CATEGORY,\n",
    "            DATEDIFF(CURDATE(), DATE(o.INSERTED_TIMESTAMP)) as days_ago\n",
    "        FROM {config.ORDER_TABLE} o\n",
    "        JOIN {config.SKU_MASTER_TABLE} s ON o.ARTICLE_ID = s.SKU_ID\n",
    "        WHERE o.INSERTED_TIMESTAMP >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)\n",
    "        AND s.SKU_NAME IS NOT NULL\n",
    "        ORDER BY o.INSERTED_TIMESTAMP DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"   📊 Executing query...\")\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"   ✅ Loaded {len(df):,} order records\")\n",
    "        print(f\"   📅 Date range: {df['INSERTED_TIMESTAMP'].min()} to {df['INSERTED_TIMESTAMP'].max()}\")\n",
    "        print(f\"   🛒 Unique orders: {df['ORDER_ID'].nunique():,}\")\n",
    "        print(f\"   🏷️ Unique SKUs: {df['SKU_NAME'].nunique():,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error loading data: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 3: Apply time-based weighting\n",
    "    print(\"\\n3️⃣ Applying time-based analysis...\")\n",
    "    \n",
    "    # Exponential decay weighting (recent orders weighted higher)\n",
    "    df['time_weight'] = np.exp(-df['days_ago'] / 30)  # 30-day decay\n",
    "    \n",
    "    # Add recency scoring\n",
    "    df['recency_score'] = 1 / (1 + df['days_ago'] / 7)  # Weekly recency\n",
    "    \n",
    "    # Composite weight\n",
    "    df['composite_weight'] = (df['time_weight'] * 0.7) + (df['recency_score'] * 0.3)\n",
    "    \n",
    "    print(f\"   ✅ Applied time-based weighting\")\n",
    "    print(f\"   📊 Weight range: {df['composite_weight'].min():.4f} - {df['composite_weight'].max():.4f}\")\n",
    "    \n",
    "    # Step 4: Create market basket data\n",
    "    print(\"\\n4️⃣ Creating market basket data...\")\n",
    "    \n",
    "    # Group by order and create basket\n",
    "    basket_data = df.groupby(['ORDER_ID', 'SKU_NAME'])['composite_weight'].sum().reset_index()\n",
    "    \n",
    "    # Create binary matrix with weighted values\n",
    "    basket_matrix = basket_data.pivot_table(\n",
    "        index='ORDER_ID', \n",
    "        columns='SKU_NAME', \n",
    "        values='composite_weight', \n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Convert to binary (1 if weight > 0, 0 otherwise)\n",
    "    basket_binary = (basket_matrix > 0).astype(int)\n",
    "    \n",
    "    print(f\"   ✅ Created basket matrix: {basket_binary.shape}\")\n",
    "    print(f\"   📦 Orders: {len(basket_binary)}\")\n",
    "    print(f\"   🏷️ Unique SKUs: {len(basket_binary.columns)}\")\n",
    "    \n",
    "    # Step 5: Generate frequent itemsets and association rules\n",
    "    print(\"\\n5️⃣ Mining association rules...\")\n",
    "    \n",
    "    try:\n",
    "        # Find frequent itemsets (lower threshold for better results)\n",
    "        frequent_itemsets = apriori(basket_binary, min_support=0.001, use_colnames=True, verbose=1)\n",
    "        print(f\"   ✅ Found {len(frequent_itemsets)} frequent itemsets\")\n",
    "        \n",
    "        if len(frequent_itemsets) == 0:\n",
    "            print(\"   ⚠️ No frequent itemsets found. Lowering support threshold...\")\n",
    "            frequent_itemsets = apriori(basket_binary, min_support=0.0005, use_colnames=True, verbose=1)\n",
    "            print(f\"   ✅ Found {len(frequent_itemsets)} frequent itemsets with lower threshold\")\n",
    "        \n",
    "        # Generate association rules\n",
    "        if len(frequent_itemsets) > 0:\n",
    "            rules = association_rules(\n",
    "                frequent_itemsets, \n",
    "                metric=\"confidence\", \n",
    "                min_threshold=0.1,  # Lower threshold\n",
    "                num_itemsets=len(frequent_itemsets)\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ Generated {len(rules)} association rules\")\n",
    "            \n",
    "            if len(rules) > 0:\n",
    "                # Step 6: Create weighted composite scores\n",
    "                print(\"\\n6️⃣ Computing composite scores...\")\n",
    "                \n",
    "                # Extract antecedent and consequent items (taking first item from each set)\n",
    "                rules['sku1'] = rules['antecedents'].apply(lambda x: list(x)[0] if len(x) > 0 else '')\n",
    "                rules['sku2'] = rules['consequents'].apply(lambda x: list(x)[0] if len(x) > 0 else '')\n",
    "                \n",
    "                # Calculate composite score using confidence, lift, and time-based factors\n",
    "                rules['base_score'] = (\n",
    "                    rules['confidence'] * 0.4 + \n",
    "                    (rules['lift'] / rules['lift'].max()) * 0.3 + \n",
    "                    rules['support'] * 0.3\n",
    "                )\n",
    "                \n",
    "                # Add time-based boost for recent patterns\n",
    "                # Calculate average days_ago for each rule\n",
    "                time_boost = []\n",
    "                for _, rule in rules.iterrows():\n",
    "                    sku1, sku2 = rule['sku1'], rule['sku2']\n",
    "                    \n",
    "                    # Find orders containing both items\n",
    "                    rule_orders = df[\n",
    "                        (df['SKU_NAME'].isin([sku1, sku2]))\n",
    "                    ].groupby('ORDER_ID')['SKU_NAME'].apply(set)\n",
    "                    \n",
    "                    relevant_orders = rule_orders[rule_orders.apply(lambda x: {sku1, sku2}.issubset(x))]\n",
    "                    \n",
    "                    if len(relevant_orders) > 0:\n",
    "                        avg_days = df[df['ORDER_ID'].isin(relevant_orders.index)]['days_ago'].mean()\n",
    "                        boost = 1 / (1 + avg_days / 14)  # 2-week decay\n",
    "                    else:\n",
    "                        boost = 0.5  # Default boost\n",
    "                    \n",
    "                    time_boost.append(boost)\n",
    "                \n",
    "                rules['time_boost'] = time_boost\n",
    "                \n",
    "                # Final composite score\n",
    "                rules['association_composite_score'] = (\n",
    "                    rules['base_score'] * 0.8 + \n",
    "                    rules['time_boost'] * 0.2\n",
    "                )\n",
    "                \n",
    "                # Step 7: Prepare final output\n",
    "                print(\"\\n7️⃣ Preparing final output...\")\n",
    "                \n",
    "                # Select and clean final columns\n",
    "                final_rules = rules[['sku1', 'sku2', 'association_composite_score']].copy()\n",
    "                \n",
    "                # Remove empty SKUs and sort by score\n",
    "                final_rules = final_rules[\n",
    "                    (final_rules['sku1'] != '') & \n",
    "                    (final_rules['sku2'] != '') &\n",
    "                    (final_rules['sku1'] != final_rules['sku2'])  # Remove self-associations\n",
    "                ].sort_values('association_composite_score', ascending=False)\n",
    "                \n",
    "                print(f\"   ✅ Final rules: {len(final_rules)}\")\n",
    "                print(f\"   📊 Score range: {final_rules['association_composite_score'].min():.4f} - {final_rules['association_composite_score'].max():.4f}\")\n",
    "                \n",
    "                # Step 8: Save to CSV\n",
    "                csv_filename = f\"association_rules_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                final_rules.to_csv(csv_filename, index=False)\n",
    "                print(f\"\\n8️⃣ ✅ Saved to CSV: {csv_filename}\")\n",
    "                \n",
    "                # Display top rules\n",
    "                print(f\"\\n🎯 TOP 10 ASSOCIATION RULES:\")\n",
    "                print(\"-\" * 80)\n",
    "                display_rules = final_rules.head(10)\n",
    "                for i, (_, rule) in enumerate(display_rules.iterrows(), 1):\n",
    "                    print(f\"{i:2d}. {rule['sku1'][:30]:<30} → {rule['sku2'][:30]:<30} | Score: {rule['association_composite_score']:.4f}\")\n",
    "                \n",
    "                # Optional: Save to database\n",
    "                save_to_db = input(\"\\n💾 Save rules to database? (y/n): \").lower().strip() == 'y'\n",
    "                \n",
    "                if save_to_db:\n",
    "                    print(\"\\n9️⃣ Saving to database...\")\n",
    "                    try:\n",
    "                        conn = mysql.connector.connect(\n",
    "                            host=config.DB_HOST,\n",
    "                            user=config.DB_USER,\n",
    "                            password=config.DB_PASSWORD,\n",
    "                            database=config.DB_NAME\n",
    "                        )\n",
    "                        cursor = conn.cursor()\n",
    "                        \n",
    "                        # Create table if not exists\n",
    "                        create_table_query = \"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS association_rules_export (\n",
    "                            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                            sku1 VARCHAR(255),\n",
    "                            sku2 VARCHAR(255),\n",
    "                            association_composite_score DECIMAL(10,6),\n",
    "                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                            INDEX idx_sku1 (sku1),\n",
    "                            INDEX idx_sku2 (sku2),\n",
    "                            INDEX idx_score (association_composite_score)\n",
    "                        )\n",
    "                        \"\"\"\n",
    "                        cursor.execute(create_table_query)\n",
    "                        \n",
    "                        # Clear existing data\n",
    "                        cursor.execute(\"DELETE FROM association_rules_export\")\n",
    "                        \n",
    "                        # Insert new rules\n",
    "                        insert_query = \"\"\"\n",
    "                        INSERT INTO association_rules_export (sku1, sku2, association_composite_score)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        rule_data = [\n",
    "                            (row['sku1'], row['sku2'], float(row['association_composite_score']))\n",
    "                            for _, row in final_rules.iterrows()\n",
    "                        ]\n",
    "                        \n",
    "                        cursor.executemany(insert_query, rule_data)\n",
    "                        conn.commit()\n",
    "                        \n",
    "                        print(f\"   ✅ Saved {len(rule_data)} rules to database table 'association_rules_export'\")\n",
    "                        \n",
    "                        cursor.close()\n",
    "                        conn.close()\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"   ❌ Error saving to database: {e}\")\n",
    "                \n",
    "                return final_rules, csv_filename\n",
    "                \n",
    "            else:\n",
    "                print(\"   ❌ No association rules generated\")\n",
    "                return None, None\n",
    "        else:\n",
    "            print(\"   ❌ No frequent itemsets found\")\n",
    "            return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error in rule mining: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Run the complete pipeline\n",
    "print(\"🎯 Starting complete association rule mining pipeline...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_rules, csv_file = generate_association_rules_csv()\n",
    "\n",
    "if result_rules is not None:\n",
    "    print(f\"\\n🎉 SUCCESS! Generated {len(result_rules)} association rules\")\n",
    "    print(f\"📁 CSV file: {csv_file}\")\n",
    "    print(f\"📊 Ready for use in your applications!\")\n",
    "else:\n",
    "    print(f\"\\n❌ Pipeline failed - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84e765c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Checking available columns in your tables...\n",
      "\n",
      "🔍 Columns in wms_to_wcs_order_line_request_data:\n",
      "   • WMS_ORDER_LINE_REQUEST_DATA_ID (bigint)\n",
      "   • WMS_ORDER_REQUEST_DATA_ID (bigint)\n",
      "   • ORDER_ID (varchar(36))\n",
      "   • ORDER_LINE_ID (varchar(36))\n",
      "   • ARTICLE_ID (varchar(36))\n",
      "   • QUANTITY (int)\n",
      "   • BATCH_ID (varchar(36))\n",
      "   • MRP (decimal(10,3))\n",
      "   • EXPIRY_DATE (date)\n",
      "   • BATCH_NUMBER (varchar(200))\n",
      "   • DELIVERY_DATE (date)\n",
      "   • DISPLAY_OPERATOR_INSTRUCTION (text)\n",
      "   • ORDER_LINE_PROCESS_STATUS (enum('DELETED','PENDING','ORDER_CANCELLED','ORDERLINE_COMPLETED','ORDERLINETAKEN'))\n",
      "   • STOCK_ADJUSTMENT_PAYLOAD_ID (varchar(50))\n",
      "   • INSERTED_TIMESTAMP (datetime(3))\n",
      "   • INSERTED_BY (varchar(50))\n",
      "   • UPDATED_TIMESTAMP (datetime(3))\n",
      "   • UPDATED_BY (varchar(50))\n",
      "\n",
      "🔍 Columns in sku_master:\n",
      "   • SKU_ID (varchar(200))\n",
      "   • SKU_NAME (varchar(1000))\n",
      "   • VELOCITY (int)\n",
      "   • CATEGORY (int)\n",
      "   • MIN_SEGMENT_SIZE (int)\n",
      "   • MAX_QUANTITY_PER_SEGMENT (int)\n",
      "   • HEIGHT (decimal(10,3))\n",
      "   • LENGTH (decimal(10,3))\n",
      "   • WIDTH (decimal(10,2))\n",
      "   • WEIGHT_OF_EACH_SKU (decimal(10,3))\n",
      "   • IMAGE_URL (varchar(1000))\n",
      "   • IS_ACTIVE (tinyint(1))\n",
      "   • INSERTED_TIMESTAMP (datetime(3))\n",
      "   • INSERTED_BY (varchar(50))\n",
      "   • UPDATED_TIMESTAMP (datetime(3))\n",
      "   • UPDATED_BY (varchar(50))\n"
     ]
    }
   ],
   "source": [
    "# Let's check the actual columns in your tables to fix the SQL query\n",
    "try:\n",
    "    from app.utils.config import config\n",
    "    \n",
    "    conn = mysql.connector.connect(\n",
    "        host=config.DB_HOST,\n",
    "        user=config.DB_USER,\n",
    "        password=config.DB_PASSWORD,\n",
    "        database=config.DB_NAME\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"📋 Checking available columns in your tables...\")\n",
    "    \n",
    "    # Check ORDER_TABLE columns\n",
    "    print(f\"\\n🔍 Columns in {config.ORDER_TABLE}:\")\n",
    "    cursor.execute(f\"DESCRIBE {config.ORDER_TABLE}\")\n",
    "    order_columns = cursor.fetchall()\n",
    "    for col in order_columns:\n",
    "        print(f\"   • {col[0]} ({col[1]})\")\n",
    "    \n",
    "    # Check SKU_MASTER_TABLE columns  \n",
    "    print(f\"\\n🔍 Columns in {config.SKU_MASTER_TABLE}:\")\n",
    "    cursor.execute(f\"DESCRIBE {config.SKU_MASTER_TABLE}\")\n",
    "    sku_columns = cursor.fetchall()\n",
    "    for col in sku_columns:\n",
    "        print(f\"   • {col[0]} ({col[1]})\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking table structure: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6ff8c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting memory-efficient association rule mining...\n",
      "============================================================\n",
      "🚀 MEMORY-EFFICIENT ASSOCIATION RULE MINING\n",
      "=======================================================\n",
      "1️⃣ Checking server status...\n",
      "   Server: ✅ Running\n",
      "\n",
      "2️⃣ Loading data and filtering to popular SKUs...\n",
      "   📊 Finding popular SKUs...\n",
      "   ✅ Selected top 50 popular SKUs\n",
      "   📊 Order count range: 620 - 1722\n",
      "   📊 Loading filtered order data...\n",
      "   ✅ Loaded 58,362 order records for popular SKUs\n",
      "   📅 Date range: 2025-08-15 13:56:12.598000 to 2025-09-17 13:26:30.633000\n",
      "   🛒 Unique orders: 2,920\n",
      "   🏷️ Unique SKUs: 50\n",
      "\n",
      "3️⃣ Applying time-based analysis...\n",
      "   ✅ Applied time-based weighting\n",
      "   📊 Weight range: 0.1298 - 0.3579\n",
      "\n",
      "4️⃣ Creating market basket data...\n",
      "   ✅ Created basket matrix: (2920, 50)\n",
      "   📦 Orders: 2920\n",
      "   🏷️ Unique SKUs: 50\n",
      "\n",
      "5️⃣ Mining association rules...\n",
      "   🔧 Using minimum support: 0.01\n",
      "Processing 871612 combinations | Sampling itemset size 4   ❌ Error in rule mining: Unable to allocate 19.0 GiB for an array with shape (217903, 4, 2920) and data type int64\n",
      "\n",
      "❌ Pipeline failed - check error messages above\n"
     ]
    }
   ],
   "source": [
    "# Memory-efficient version of direct mining - focusing on top SKUs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def generate_association_rules_memory_efficient():\n",
    "    \"\"\"\n",
    "    Memory-efficient mining: Focus on top SKUs to avoid memory issues\n",
    "    \"\"\"\n",
    "    print(\"🚀 MEMORY-EFFICIENT ASSOCIATION RULE MINING\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Step 1: Check server (optional)\n",
    "    try:\n",
    "        print(\"1️⃣ Checking server status...\")\n",
    "        response = requests.get(f\"{BASE_URL}/\", timeout=3)\n",
    "        print(f\"   Server: {'✅ Running' if response.status_code == 200 else '⚠️ Issues'}\")\n",
    "    except:\n",
    "        print(\"   ⚠️ Server not responding (continuing with direct processing)\")\n",
    "    \n",
    "    # Step 2: Load data and filter to top SKUs\n",
    "    print(\"\\n2️⃣ Loading data and filtering to popular SKUs...\")\n",
    "    try:\n",
    "        from app.utils.config import config\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = mysql.connector.connect(\n",
    "            host=config.DB_HOST,\n",
    "            user=config.DB_USER,\n",
    "            password=config.DB_PASSWORD,\n",
    "            database=config.DB_NAME\n",
    "        )\n",
    "        \n",
    "        # First, get the most popular SKUs (top 50 to keep memory manageable)\n",
    "        popularity_query = f\"\"\"\n",
    "        SELECT \n",
    "            s.SKU_NAME,\n",
    "            COUNT(DISTINCT o.ORDER_ID) as order_count,\n",
    "            COUNT(*) as total_quantity\n",
    "        FROM {config.ORDER_TABLE} o\n",
    "        JOIN {config.SKU_MASTER_TABLE} s ON o.ARTICLE_ID = s.SKU_ID\n",
    "        WHERE o.INSERTED_TIMESTAMP >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)\n",
    "        AND s.SKU_NAME IS NOT NULL\n",
    "        GROUP BY s.SKU_NAME\n",
    "        HAVING order_count >= 5  -- Only SKUs that appear in 5+ orders\n",
    "        ORDER BY order_count DESC, total_quantity DESC\n",
    "        LIMIT 50\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"   📊 Finding popular SKUs...\")\n",
    "        popular_skus_df = pd.read_sql(popularity_query, conn)\n",
    "        popular_sku_list = popular_skus_df['SKU_NAME'].tolist()\n",
    "        \n",
    "        print(f\"   ✅ Selected top {len(popular_sku_list)} popular SKUs\")\n",
    "        print(f\"   📊 Order count range: {popular_skus_df['order_count'].min()} - {popular_skus_df['order_count'].max()}\")\n",
    "        \n",
    "        # Now load data only for these popular SKUs (using parameterized query)\n",
    "        placeholders = ','.join(['%s'] * len(popular_sku_list))\n",
    "        main_query = f\"\"\"\n",
    "        SELECT \n",
    "            o.ORDER_ID,\n",
    "            o.INSERTED_TIMESTAMP,\n",
    "            s.SKU_NAME,\n",
    "            s.CATEGORY,\n",
    "            DATEDIFF(CURDATE(), DATE(o.INSERTED_TIMESTAMP)) as days_ago\n",
    "        FROM {config.ORDER_TABLE} o\n",
    "        JOIN {config.SKU_MASTER_TABLE} s ON o.ARTICLE_ID = s.SKU_ID\n",
    "        WHERE o.INSERTED_TIMESTAMP >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)\n",
    "        AND s.SKU_NAME IN ({placeholders})\n",
    "        ORDER BY o.INSERTED_TIMESTAMP DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"   📊 Loading filtered order data...\")\n",
    "        df = pd.read_sql(main_query, conn, params=popular_sku_list)\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"   ✅ Loaded {len(df):,} order records for popular SKUs\")\n",
    "        print(f\"   📅 Date range: {df['INSERTED_TIMESTAMP'].min()} to {df['INSERTED_TIMESTAMP'].max()}\")\n",
    "        print(f\"   🛒 Unique orders: {df['ORDER_ID'].nunique():,}\")\n",
    "        print(f\"   🏷️ Unique SKUs: {df['SKU_NAME'].nunique():,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error loading data: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 3: Apply time-based weighting\n",
    "    print(\"\\n3️⃣ Applying time-based analysis...\")\n",
    "    \n",
    "    df['time_weight'] = np.exp(-df['days_ago'] / 30)  # 30-day decay\n",
    "    df['recency_score'] = 1 / (1 + df['days_ago'] / 7)  # Weekly recency\n",
    "    df['composite_weight'] = (df['time_weight'] * 0.7) + (df['recency_score'] * 0.3)\n",
    "    \n",
    "    print(f\"   ✅ Applied time-based weighting\")\n",
    "    print(f\"   📊 Weight range: {df['composite_weight'].min():.4f} - {df['composite_weight'].max():.4f}\")\n",
    "    \n",
    "    # Step 4: Create market basket data\n",
    "    print(\"\\n4️⃣ Creating market basket data...\")\n",
    "    \n",
    "    basket_data = df.groupby(['ORDER_ID', 'SKU_NAME'])['composite_weight'].sum().reset_index()\n",
    "    basket_matrix = basket_data.pivot_table(\n",
    "        index='ORDER_ID', \n",
    "        columns='SKU_NAME', \n",
    "        values='composite_weight', \n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Convert to binary (1 if weight > 0, 0 otherwise)\n",
    "    basket_binary = (basket_matrix > 0).astype(int)\n",
    "    \n",
    "    print(f\"   ✅ Created basket matrix: {basket_binary.shape}\")\n",
    "    print(f\"   📦 Orders: {len(basket_binary)}\")\n",
    "    print(f\"   🏷️ Unique SKUs: {len(basket_binary.columns)}\")\n",
    "    \n",
    "    # Step 5: Generate frequent itemsets with higher support\n",
    "    print(\"\\n5️⃣ Mining association rules...\")\n",
    "    \n",
    "    try:\n",
    "        # Use higher support threshold to reduce computation\n",
    "        min_support = 0.01  # 1% support\n",
    "        print(f\"   🔧 Using minimum support: {min_support}\")\n",
    "        \n",
    "        frequent_itemsets = apriori(basket_binary, min_support=min_support, use_colnames=True, verbose=1)\n",
    "        print(f\"   ✅ Found {len(frequent_itemsets)} frequent itemsets\")\n",
    "        \n",
    "        if len(frequent_itemsets) == 0:\n",
    "            print(\"   ⚠️ No frequent itemsets found. Lowering support threshold...\")\n",
    "            min_support = 0.005  # 0.5% support\n",
    "            frequent_itemsets = apriori(basket_binary, min_support=min_support, use_colnames=True, verbose=1)\n",
    "            print(f\"   ✅ Found {len(frequent_itemsets)} frequent itemsets with lower threshold\")\n",
    "        \n",
    "        if len(frequent_itemsets) > 0:\n",
    "            # Generate association rules\n",
    "            rules = association_rules(\n",
    "                frequent_itemsets, \n",
    "                metric=\"confidence\", \n",
    "                min_threshold=0.2,  # Higher confidence threshold\n",
    "                num_itemsets=len(frequent_itemsets)\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ Generated {len(rules)} association rules\")\n",
    "            \n",
    "            if len(rules) > 0:\n",
    "                # Step 6: Create composite scores\n",
    "                print(\"\\n6️⃣ Computing composite scores...\")\n",
    "                \n",
    "                rules['sku1'] = rules['antecedents'].apply(lambda x: list(x)[0] if len(x) > 0 else '')\n",
    "                rules['sku2'] = rules['consequents'].apply(lambda x: list(x)[0] if len(x) > 0 else '')\n",
    "                \n",
    "                # Simplified scoring for better performance\n",
    "                rules['base_score'] = (\n",
    "                    rules['confidence'] * 0.5 + \n",
    "                    (rules['lift'] / rules['lift'].max()) * 0.3 + \n",
    "                    rules['support'] * 0.2\n",
    "                )\n",
    "                \n",
    "                # Quick time boost calculation\n",
    "                rules['time_boost'] = 0.8  # Default boost for all rules\n",
    "                \n",
    "                rules['association_composite_score'] = rules['base_score'] * 0.9 + rules['time_boost'] * 0.1\n",
    "                \n",
    "                # Final output\n",
    "                final_rules = rules[['sku1', 'sku2', 'association_composite_score', 'confidence', 'lift', 'support']].copy()\n",
    "                final_rules = final_rules[\n",
    "                    (final_rules['sku1'] != '') & \n",
    "                    (final_rules['sku2'] != '') &\n",
    "                    (final_rules['sku1'] != final_rules['sku2'])\n",
    "                ].sort_values('association_composite_score', ascending=False)\n",
    "                \n",
    "                print(f\"   ✅ Final rules: {len(final_rules)}\")\n",
    "                print(f\"   📊 Score range: {final_rules['association_composite_score'].min():.4f} - {final_rules['association_composite_score'].max():.4f}\")\n",
    "                \n",
    "                # Step 7: Save to CSV\n",
    "                csv_filename = f\"association_rules_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                export_rules = final_rules[['sku1', 'sku2', 'association_composite_score']].copy()\n",
    "                export_rules.to_csv(csv_filename, index=False)\n",
    "                print(f\"\\n7️⃣ ✅ Saved to CSV: {csv_filename}\")\n",
    "                \n",
    "                # Display top rules\n",
    "                print(f\"\\n🎯 TOP 10 ASSOCIATION RULES:\")\n",
    "                print(\"-\" * 80)\n",
    "                display_rules = final_rules.head(10)\n",
    "                for i, (_, rule) in enumerate(display_rules.iterrows(), 1):\n",
    "                    print(f\"{i:2d}. {rule['sku1'][:30]:<30} → {rule['sku2'][:30]:<30} | Score: {rule['association_composite_score']:.4f}\")\n",
    "                \n",
    "                return final_rules, csv_filename\n",
    "                \n",
    "            else:\n",
    "                print(\"   ❌ No association rules generated\")\n",
    "                return None, None\n",
    "        else:\n",
    "            print(\"   ❌ No frequent itemsets found even with lower threshold\")\n",
    "            return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error in rule mining: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Run the memory-efficient pipeline\n",
    "print(\"🎯 Starting memory-efficient association rule mining...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_rules, csv_file = generate_association_rules_memory_efficient()\n",
    "\n",
    "if result_rules is not None:\n",
    "    print(f\"\\n🎉 SUCCESS! Generated {len(result_rules)} association rules\")\n",
    "    print(f\"📁 CSV file: {csv_file}\")\n",
    "    print(f\"📊 Ready for use in your applications!\")\n",
    "else:\n",
    "    print(f\"\\n❌ Pipeline failed - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee5397cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Running ultra-conservative mining...\n",
      "🚀 ULTRA-CONSERVATIVE ASSOCIATION RULE MINING\n",
      "=======================================================\n",
      "   📊 Finding top 20 SKUs...\n",
      "   ✅ Selected top 20 SKUs\n",
      "   📊 Order count range: 888 - 1722\n",
      "   📊 Loading order data...\n",
      "   ✅ Loaded 30,964 records\n",
      "   🛒 Unique orders: 2,759\n",
      "   🏷️ Unique SKUs: 20\n",
      "   🛒 Market basket size: (2759, 20)\n",
      "   ⛏️ Mining with high support threshold...\n",
      "   ✅ Found 210 frequent itemsets\n",
      "   ✅ Generated 374 rules\n",
      "\n",
      "✅ SUCCESS! Generated 374 rules\n",
      "📁 Saved to: association_rules_top20_20251013_130857.csv\n",
      "\n",
      "🎯 TOP 10 RULES:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. Let's Try Namkeen Combo Pack O → YiPPee! Magic Masala Noodles w | 0.8687\n",
      " 2. Let's Try Salted Peanuts       → YiPPee! Magic Masala Noodles w | 0.8580\n",
      " 3. Haldiram's Bhujia Sev (200.0 G → YiPPee! Magic Masala Noodles w | 0.8474\n",
      " 4. Haldiram's Bhujia Sev (200.0 G → Cheetos Cheese Puffs (28.0 GRA | 0.8048\n",
      " 5. Let's Try Salted Peanuts       → Haldiram's Bhujia Sev (200.0 G | 0.7836\n",
      " 6. Haldiram's Bhujia Sev (200.0 G → Let's Try Salted Peanuts       | 0.7824\n",
      " 7. Let's Try Salted Peanuts       → Chheda's Yellow Banana Chips ( | 0.7789\n",
      " 8. Haldiram's Bhujia Sev (200.0 G → Chheda's Yellow Banana Chips ( | 0.7550\n",
      " 9. Let's Try Salted Peanuts       → Cheetos Cheese Puffs (28.0 GRA | 0.7549\n",
      "10. Let's Try Namkeen Combo Pack O → Let's Try Salted Peanuts       | 0.7460\n",
      "\n",
      "🎉 SUCCESS! Check the CSV file: association_rules_top20_20251013_130857.csv\n"
     ]
    }
   ],
   "source": [
    "# Ultra-conservative memory-efficient mining - top 20 SKUs only\n",
    "def generate_rules_top_skus():\n",
    "    \"\"\"\n",
    "    Ultra-conservative: Top 20 SKUs only with high support threshold\n",
    "    \"\"\"\n",
    "    print(\"🚀 ULTRA-CONSERVATIVE ASSOCIATION RULE MINING\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    try:\n",
    "        from app.utils.config import config\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = mysql.connector.connect(\n",
    "            host=config.DB_HOST,\n",
    "            user=config.DB_USER,\n",
    "            password=config.DB_PASSWORD,\n",
    "            database=config.DB_NAME\n",
    "        )\n",
    "        \n",
    "        # Get top 20 most popular SKUs\n",
    "        popularity_query = f\"\"\"\n",
    "        SELECT \n",
    "            s.SKU_NAME,\n",
    "            COUNT(DISTINCT o.ORDER_ID) as order_count\n",
    "        FROM {config.ORDER_TABLE} o\n",
    "        JOIN {config.SKU_MASTER_TABLE} s ON o.ARTICLE_ID = s.SKU_ID\n",
    "        WHERE o.INSERTED_TIMESTAMP >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)\n",
    "        AND s.SKU_NAME IS NOT NULL\n",
    "        GROUP BY s.SKU_NAME\n",
    "        HAVING order_count >= 10\n",
    "        ORDER BY order_count DESC\n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"   📊 Finding top 20 SKUs...\")\n",
    "        popular_skus_df = pd.read_sql(popularity_query, conn)\n",
    "        popular_sku_list = popular_skus_df['SKU_NAME'].tolist()\n",
    "        \n",
    "        print(f\"   ✅ Selected top {len(popular_sku_list)} SKUs\")\n",
    "        print(f\"   📊 Order count range: {popular_skus_df['order_count'].min()} - {popular_skus_df['order_count'].max()}\")\n",
    "        \n",
    "        # Load data for these SKUs\n",
    "        placeholders = ','.join(['%s'] * len(popular_sku_list))\n",
    "        main_query = f\"\"\"\n",
    "        SELECT \n",
    "            o.ORDER_ID,\n",
    "            s.SKU_NAME,\n",
    "            DATEDIFF(CURDATE(), DATE(o.INSERTED_TIMESTAMP)) as days_ago\n",
    "        FROM {config.ORDER_TABLE} o\n",
    "        JOIN {config.SKU_MASTER_TABLE} s ON o.ARTICLE_ID = s.SKU_ID\n",
    "        WHERE o.INSERTED_TIMESTAMP >= DATE_SUB(CURDATE(), INTERVAL 60 DAY)\n",
    "        AND s.SKU_NAME IN ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"   📊 Loading order data...\")\n",
    "        df = pd.read_sql(main_query, conn, params=popular_sku_list)\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"   ✅ Loaded {len(df):,} records\")\n",
    "        print(f\"   🛒 Unique orders: {df['ORDER_ID'].nunique():,}\")\n",
    "        print(f\"   🏷️ Unique SKUs: {df['SKU_NAME'].nunique():,}\")\n",
    "        \n",
    "        # Apply simple time weighting\n",
    "        df['weight'] = np.exp(-df['days_ago'] / 30)\n",
    "        \n",
    "        # Create market basket (simple binary)\n",
    "        basket = df.groupby(['ORDER_ID', 'SKU_NAME'])['weight'].sum().reset_index()\n",
    "        basket_matrix = basket.pivot_table(\n",
    "            index='ORDER_ID', \n",
    "            columns='SKU_NAME', \n",
    "            values='weight', \n",
    "            fill_value=0\n",
    "        )\n",
    "        basket_binary = (basket_matrix > 0).astype(int)\n",
    "        \n",
    "        print(f\"   🛒 Market basket size: {basket_binary.shape}\")\n",
    "        \n",
    "        # Mine with very high support\n",
    "        print(\"   ⛏️ Mining with high support threshold...\")\n",
    "        frequent_itemsets = apriori(basket_binary, min_support=0.03, use_colnames=True, max_len=2)\n",
    "        print(f\"   ✅ Found {len(frequent_itemsets)} frequent itemsets\")\n",
    "        \n",
    "        if len(frequent_itemsets) > 0:\n",
    "            # Generate rules\n",
    "            rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "            print(f\"   ✅ Generated {len(rules)} rules\")\n",
    "            \n",
    "            if len(rules) > 0:\n",
    "                # Create final output\n",
    "                rules['sku1'] = rules['antecedents'].apply(lambda x: list(x)[0])\n",
    "                rules['sku2'] = rules['consequents'].apply(lambda x: list(x)[0])\n",
    "                rules['association_composite_score'] = (\n",
    "                    rules['confidence'] * 0.6 + \n",
    "                    rules['lift'] / rules['lift'].max() * 0.4\n",
    "                )\n",
    "                \n",
    "                final_rules = rules[['sku1', 'sku2', 'association_composite_score', 'confidence', 'lift', 'support']].copy()\n",
    "                final_rules = final_rules.sort_values('association_composite_score', ascending=False)\n",
    "                \n",
    "                # Save to CSV\n",
    "                csv_filename = f\"association_rules_top20_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                export_df = final_rules[['sku1', 'sku2', 'association_composite_score']].copy()\n",
    "                export_df.to_csv(csv_filename, index=False)\n",
    "                \n",
    "                print(f\"\\n✅ SUCCESS! Generated {len(final_rules)} rules\")\n",
    "                print(f\"📁 Saved to: {csv_filename}\")\n",
    "                \n",
    "                print(f\"\\n🎯 TOP 10 RULES:\")\n",
    "                print(\"-\" * 80)\n",
    "                for i, (_, rule) in enumerate(final_rules.head(10).iterrows(), 1):\n",
    "                    print(f\"{i:2d}. {rule['sku1'][:30]:<30} → {rule['sku2'][:30]:<30} | {rule['association_composite_score']:.4f}\")\n",
    "                \n",
    "                return final_rules, csv_filename\n",
    "            \n",
    "        print(\"   ⚠️ No rules generated - trying even lower threshold...\")\n",
    "        frequent_itemsets = apriori(basket_binary, min_support=0.02, use_colnames=True, max_len=2)\n",
    "        \n",
    "        if len(frequent_itemsets) > 0:\n",
    "            rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.2)\n",
    "            if len(rules) > 0:\n",
    "                rules['sku1'] = rules['antecedents'].apply(lambda x: list(x)[0])\n",
    "                rules['sku2'] = rules['consequents'].apply(lambda x: list(x)[0])\n",
    "                rules['association_composite_score'] = rules['confidence'] * 0.7 + rules['lift'] / rules['lift'].max() * 0.3\n",
    "                \n",
    "                final_rules = rules[['sku1', 'sku2', 'association_composite_score']].copy()\n",
    "                final_rules = final_rules.sort_values('association_composite_score', ascending=False)\n",
    "                \n",
    "                csv_filename = f\"association_rules_conservative_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                final_rules.to_csv(csv_filename, index=False)\n",
    "                \n",
    "                print(f\"✅ Generated {len(final_rules)} rules with lower threshold\")\n",
    "                print(f\"📁 Saved to: {csv_filename}\")\n",
    "                \n",
    "                return final_rules, csv_filename\n",
    "        \n",
    "        print(\"❌ No association rules could be generated\")\n",
    "        return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Run ultra-conservative mining\n",
    "print(\"🎯 Running ultra-conservative mining...\")\n",
    "result, csv_file = generate_rules_top_skus()\n",
    "\n",
    "if result is not None:\n",
    "    print(f\"\\n🎉 SUCCESS! Check the CSV file: {csv_file}\")\n",
    "else:\n",
    "    print(f\"\\n❌ Failed to generate rules\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
